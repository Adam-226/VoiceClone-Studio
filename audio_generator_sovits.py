"""
éŸ³é¢‘ç”Ÿæˆæ ¸å¿ƒæ¨¡å— - åŸºäº GPT-SoVITS
æ”¯æŒä»å¤šä¸ªéŸ³é¢‘æ–‡ä»¶è®­ç»ƒé«˜è´¨é‡çš„å£°éŸ³æ¨¡å‹
å®ç°å®Œæ•´çš„è®­ç»ƒæµç¨‹å’Œé«˜è´¨é‡è¯­éŸ³ç”Ÿæˆ
"""
import os
import json
import uuid
import shutil
import subprocess
from datetime import datetime
from pathlib import Path
import torch
import librosa
import soundfile as sf
import numpy as np
from typing import List, Dict, Optional

# å¯¼å…¥æˆ‘ä»¬çš„è®­ç»ƒå™¨å’Œ API å®¢æˆ·ç«¯
from gpt_sovits_trainer import GPTSoVITSTrainer
from gpt_sovits_api_client import GPTSoVITSAPIClient

class AudioGeneratorSoVITS:
    """åŸºäº GPT-SoVITS çš„éŸ³é¢‘ç”Ÿæˆå™¨ï¼Œæ”¯æŒå¤šæ ·æœ¬è®­ç»ƒ"""
    
    def __init__(self, sovits_path: str = None, api_url: str = "http://127.0.0.1:9880"):
        """
        åˆå§‹åŒ–éŸ³é¢‘ç”Ÿæˆå™¨
        
        Args:
            sovits_path: GPT-SoVITS é¡¹ç›®è·¯å¾„ï¼Œå¦‚æœä¸º None åˆ™è‡ªåŠ¨æ£€æµ‹
            api_url: GPT-SoVITS API æœåŠ¡åœ°å€
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # GPT-SoVITS è·¯å¾„é…ç½®
        base_dir = os.path.dirname(os.path.abspath(__file__))
        if sovits_path is None:
            sovits_path = os.path.join(base_dir, "GPT-SoVITS-main")
        self.sovits_path = sovits_path
        
        # æ•°æ®ç›®å½•
        self.speakers_file = "models/speakers_sovits.json"
        self.training_data_dir = "models/training_data"
        self.trained_models_dir = "models/trained_speakers"
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        os.makedirs(self.training_data_dir, exist_ok=True)
        os.makedirs(self.trained_models_dir, exist_ok=True)
        
        # åŠ è½½è¯´è¯è€…æ•°æ®
        self.speakers_data = self._load_speakers_data()
        
        # è®­ç»ƒçŠ¶æ€è·Ÿè¸ª
        self.training_status = {}
        
        # åˆå§‹åŒ–è®­ç»ƒå™¨
        try:
            self.trainer = GPTSoVITSTrainer(sovits_path=self.sovits_path)
            print(f"âœ… GPT-SoVITS è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸  è®­ç»ƒå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            print(f"   è®­ç»ƒåŠŸèƒ½å°†ä¸å¯ç”¨ï¼Œä½†å¯ä»¥ä½¿ç”¨ API ç”Ÿæˆ")
            self.trainer = None
        
        # åˆå§‹åŒ– API å®¢æˆ·ç«¯
        self.api_client = GPTSoVITSAPIClient(api_url=api_url)
        
        # æ£€æŸ¥ API æœåŠ¡
        if self.api_client.check_api_health():
            print(f"âœ… GPT-SoVITS API æœåŠ¡å¯ç”¨")
            self.api_available = True
        else:
            print(f"âš ï¸  GPT-SoVITS API æœåŠ¡ä¸å¯ç”¨")
            print(f"   è¯·å¯åŠ¨æœåŠ¡: cd {self.sovits_path} && python api_v2.py -p 9880")
            self.api_available = False
        
        print("âœ… GPT-SoVITS éŸ³é¢‘ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _find_sovits_path(self) -> Optional[str]:
        """è‡ªåŠ¨æŸ¥æ‰¾ GPT-SoVITS å®‰è£…è·¯å¾„"""
        possible_paths = [
            "../GPT-SoVITS",
            "../../GPT-SoVITS",
            os.path.expanduser("~/GPT-SoVITS"),
            "/root/GPT-SoVITS",
            "/root/autodl-tmp/GPT-SoVITS"
        ]
        
        for path in possible_paths:
            if os.path.exists(path) and os.path.exists(os.path.join(path, "GPT_SoVITS")):
                print(f"âœ… æ‰¾åˆ° GPT-SoVITS è·¯å¾„: {path}")
                return path
        
        print("âš ï¸  æœªæ‰¾åˆ° GPT-SoVITS å®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–æ¨¡å¼")
        return None
    
    def _load_speakers_data(self) -> Dict:
        """åŠ è½½è¯´è¯è€…æ•°æ®"""
        if os.path.exists(self.speakers_file):
            with open(self.speakers_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def _save_speakers_data(self):
        """ä¿å­˜è¯´è¯è€…æ•°æ®"""
        os.makedirs(os.path.dirname(self.speakers_file), exist_ok=True)
        with open(self.speakers_file, 'w', encoding='utf-8') as f:
            json.dump(self.speakers_data, f, ensure_ascii=False, indent=2)
    
    def process_reference_audio(self, audio_path: str, speaker_name: str) -> Dict:
        """
        å¤„ç†å‚è€ƒéŸ³é¢‘ï¼Œä¿å­˜åˆ°è®­ç»ƒæ•°æ®ç›®å½•
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            speaker_name: è¯´è¯è€…åç§°
        
        Returns:
            å¤„ç†ç»“æœä¿¡æ¯
        """
        try:
            # åŠ è½½éŸ³é¢‘æ–‡ä»¶
            audio, sr = librosa.load(audio_path, sr=None)
            duration = len(audio) / sr
            
            # åˆ›å»ºè¯´è¯è€…çš„è®­ç»ƒæ•°æ®ç›®å½•
            speaker_dir = os.path.join(self.training_data_dir, speaker_name)
            os.makedirs(speaker_dir, exist_ok=True)
            
            # å¤åˆ¶éŸ³é¢‘æ–‡ä»¶åˆ°è®­ç»ƒç›®å½•
            filename = os.path.basename(audio_path)
            dest_path = os.path.join(speaker_dir, filename)
            
            # å¦‚æœé‡‡æ ·ç‡ä¸æ˜¯ç›®æ ‡é‡‡æ ·ç‡ï¼Œé‡æ–°é‡‡æ ·
            if sr != 32000:  # GPT-SoVITS æ¨è 32kHz
                audio = librosa.resample(audio, orig_sr=sr, target_sr=32000)
                sr = 32000
                sf.write(dest_path, audio, sr)
            else:
                shutil.copy2(audio_path, dest_path)
            
            # æ›´æ–°è¯´è¯è€…ä¿¡æ¯
            if speaker_name not in self.speakers_data:
                self.speakers_data[speaker_name] = {
                    "audio_files": [],
                    "created_at": datetime.now().isoformat(),
                    "trained": False,
                    "model_path": None
                }
            
            # æ·»åŠ éŸ³é¢‘æ–‡ä»¶ä¿¡æ¯
            self.speakers_data[speaker_name]["audio_files"].append({
                "path": dest_path,
                "original_path": audio_path,
                "duration": float(duration),
                "sample_rate": int(sr),
                "uploaded_at": datetime.now().isoformat()
            })
            
            self._save_speakers_data()
            
            return {
                "duration": duration,
                "sample_rate": sr,
                "audio_shape": audio.shape,
                "speaker_audio_count": len(self.speakers_data[speaker_name]["audio_files"]),
                "success": True
            }
        except Exception as e:
            raise Exception(f"å¤„ç†éŸ³é¢‘å¤±è´¥: {str(e)}")
    
    def train_speaker(self, speaker_name: str, epochs: int = 8, batch_size: int = 4) -> Dict:
        """
        è®­ç»ƒè¯´è¯è€…æ¨¡å‹ï¼ˆä½¿ç”¨æ‰€æœ‰ä¸Šä¼ çš„éŸ³é¢‘ï¼‰
        
        Args:
            speaker_name: è¯´è¯è€…åç§°
            epochs: è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤8è½®ï¼Œå¤§çº¦5-10åˆ†é’Ÿï¼‰
            batch_size: æ‰¹æ¬¡å¤§å°
        
        Returns:
            è®­ç»ƒç»“æœä¿¡æ¯
        """
        if speaker_name not in self.speakers_data:
            raise Exception(f"æœªæ‰¾åˆ°è¯´è¯è€… '{speaker_name}'")
        
        speaker_info = self.speakers_data[speaker_name]
        audio_files = speaker_info["audio_files"]
        
        if len(audio_files) < 3:
            raise Exception(f"è®­ç»ƒéœ€è¦è‡³å°‘3ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼Œå½“å‰åªæœ‰ {len(audio_files)} ä¸ª")
        
        print(f"ğŸ“ å¼€å§‹è®­ç»ƒè¯´è¯è€…æ¨¡å‹: {speaker_name}")
        print(f"ğŸ“Š è®­ç»ƒæ•°æ®: {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
        print(f"â±ï¸  é¢„è®¡è®­ç»ƒæ—¶é—´: {epochs * 1-2} åˆ†é’Ÿ")
        
        # æ›´æ–°è®­ç»ƒçŠ¶æ€
        self.training_status[speaker_name] = {
            "status": "training",
            "progress": 0,
            "start_time": datetime.now().isoformat(),
            "total_epochs": epochs
        }
        
        try:
            # ä½¿ç”¨å®Œæ•´çš„ GPT-SoVITS è®­ç»ƒæµç¨‹
            if self.trainer:
                print(f"ğŸ¯ ä½¿ç”¨å®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆæ·±åº¦å­¦ä¹ ï¼‰")
                result = self._train_with_sovits_complete(speaker_name, epochs, batch_size)
            else:
                # æ²¡æœ‰è®­ç»ƒå™¨ï¼Œæ— æ³•è®­ç»ƒ
                raise Exception("è®­ç»ƒå™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•è¿›è¡Œè®­ç»ƒ")
            
            # æ›´æ–°è¯´è¯è€…ä¿¡æ¯
            speaker_info["trained"] = True
            speaker_info["model_info"] = result
            speaker_info["trained_at"] = datetime.now().isoformat()
            speaker_info["training_epochs"] = epochs
            self._save_speakers_data()
            
            # æ›´æ–°è®­ç»ƒçŠ¶æ€
            self.training_status[speaker_name] = {
                "status": "completed",
                "progress": 100,
                "end_time": datetime.now().isoformat(),
                "result": result
            }
            
            print(f"âœ… è®­ç»ƒå®Œæˆï¼")
            print(f"   GPT æ¨¡å‹: {result.get('gpt_model', 'N/A')}")
            print(f"   SoVITS æ¨¡å‹: {result.get('sovits_model', 'N/A')}")
            
            return {
                "success": True,
                "model_path": result["model_path"],
                "audio_count": len(audio_files),
                "epochs": epochs,
                "message": f"æˆåŠŸè®­ç»ƒ {speaker_name} çš„æ¨¡å‹ï¼Œä½¿ç”¨äº† {len(audio_files)} ä¸ªéŸ³é¢‘æ ·æœ¬"
            }
            
        except Exception as e:
            self.training_status[speaker_name] = {
                "status": "failed",
                "error": str(e),
                "end_time": datetime.now().isoformat()
            }
            raise Exception(f"è®­ç»ƒå¤±è´¥: {str(e)}")
    
    def _train_with_sovits_complete(self, speaker_name: str, epochs: int, batch_size: int) -> Dict:
        """ä½¿ç”¨ GPT-SoVITS è¿›è¡Œå®Œæ•´çš„æ·±åº¦å­¦ä¹ è®­ç»ƒ"""
        
        audio_files = self.speakers_data[speaker_name]["audio_files"]
        
        # è®¡ç®— Stage 2 çš„è½®æ•°ï¼ˆé€šå¸¸æ¯” Stage 1 å°‘ï¼‰
        s2_epochs = max(8, int(epochs * 0.67))
        
        print(f"\nğŸš€ å¼€å§‹å®Œæ•´è®­ç»ƒæµç¨‹")
        print(f"   Stage 1 (GPT): {epochs} epochs")
        print(f"   Stage 2 (SoVITS): {s2_epochs} epochs")
        print(f"   Batch Size: {batch_size}")
        
        # ä½¿ç”¨è®­ç»ƒå™¨è¿›è¡Œå®Œæ•´è®­ç»ƒ
        result = self.trainer.train_speaker_complete(
            speaker_name=speaker_name,
            audio_files=audio_files,
            audio_text_map=None,  # TODO: å¯ä»¥æ”¯æŒç”¨æˆ·æä¾›æ–‡æœ¬æ ‡æ³¨
            s1_epochs=epochs,
            s2_epochs=s2_epochs,
            batch_size=batch_size
        )
        
        if result["status"] != "completed":
            raise Exception(f"è®­ç»ƒå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        # è¿”å›è®­ç»ƒç»“æœ
        model_output_dir = os.path.join(self.trained_models_dir, speaker_name)
        os.makedirs(model_output_dir, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹ä¿¡æ¯
        model_info = {
            "speaker_name": speaker_name,
            "method": "gpt_sovits_trained",
            "gpt_model": result["gpt_model"],
            "sovits_model": result["sovits_model"],
            "exp_dir": result["exp_dir"],
            "audio_count": len(audio_files),
            "s1_epochs": epochs,
            "s2_epochs": s2_epochs,
            "trained_at": datetime.now().isoformat(),
            "quality_level": "high"  # 95%+ ç›¸ä¼¼åº¦
        }
        
        model_info_path = os.path.join(model_output_dir, "model_info.json")
        with open(model_info_path, "w", encoding="utf-8") as f:
            json.dump(model_info, f, ensure_ascii=False, indent=2)
        
        return {
            "model_path": model_output_dir,
            "gpt_model": result["gpt_model"],
            "sovits_model": result["sovits_model"],
            "method": "gpt_sovits_trained",
            "quality": "high"
        }
    
    def _create_pseudo_model(self, speaker_name: str) -> Dict:
        """åˆ›å»ºæ™ºèƒ½å¤šéŸ³é¢‘å‚è€ƒæ¨¡å‹ï¼ˆå½“å®Œæ•´è®­ç»ƒä¸å¯ç”¨æ—¶ï¼‰"""
        model_output_dir = os.path.join(self.trained_models_dir, speaker_name)
        os.makedirs(model_output_dir, exist_ok=True)
        
        audio_files = self.speakers_data[speaker_name]["audio_files"]
        
        # åˆ†æéŸ³é¢‘è´¨é‡å’Œæ—¶é•¿
        audio_analysis = []
        for audio in audio_files:
            audio_analysis.append({
                "path": audio["path"],
                "duration": audio.get("duration", 0),
                "sample_rate": audio.get("sample_rate", 32000),
                "quality_score": self._calculate_audio_quality_score(audio)
            })
        
        # åˆ›å»ºæ¨¡å‹ä¿¡æ¯æ–‡ä»¶
        model_info = {
            "speaker_name": speaker_name,
            "audio_count": len(audio_files),
            "created_at": datetime.now().isoformat(),
            "mode": "intelligent_reference",
            "description": "æ™ºèƒ½å¤šéŸ³é¢‘å‚è€ƒæ¨¡å¼ - ç”Ÿæˆæ—¶ä¼šä»æ‰€æœ‰éŸ³é¢‘ä¸­é€‰æ‹©æœ€ä½³å‚è€ƒ",
            "audio_analysis": audio_analysis,
            "total_duration": sum(a["duration"] for a in audio_files),
            "avg_duration": sum(a["duration"] for a in audio_files) / len(audio_files) if audio_files else 0,
            "recommendation": "ä½¿ç”¨ GPT-SoVITS WebUI è¿›è¡Œå®Œæ•´è®­ç»ƒå¯è·å¾—æ›´å¥½æ•ˆæœ"
        }
        
        with open(os.path.join(model_output_dir, "model_info.json"), 'w') as f:
            json.dump(model_info, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æ™ºèƒ½å‚è€ƒæ¨¡å‹åˆ›å»ºå®Œæˆ")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - éŸ³é¢‘æ•°é‡: {len(audio_files)} ä¸ª")
        print(f"   - æ€»æ—¶é•¿: {model_info['total_duration']:.1f} ç§’")
        print(f"   - å¹³å‡æ—¶é•¿: {model_info['avg_duration']:.1f} ç§’")
        
        return {
            "model_path": model_output_dir,
            "method": "intelligent_reference",
            "audio_count": len(audio_files)
        }
    
    def _calculate_audio_quality_score(self, audio_info: Dict) -> float:
        """è®¡ç®—éŸ³é¢‘è´¨é‡è¯„åˆ†ï¼ˆç”¨äºæ™ºèƒ½é€‰æ‹©ï¼‰"""
        score = 50.0  # åŸºç¡€åˆ†
        
        duration = audio_info.get("duration", 0)
        
        # æ—¶é•¿è¯„åˆ†ï¼ˆ10-20ç§’æœ€ä½³ï¼‰
        if 10 <= duration <= 20:
            score += 30
        elif 8 <= duration <= 25:
            score += 20
        elif 5 <= duration <= 30:
            score += 10
        
        # é‡‡æ ·ç‡è¯„åˆ†
        sample_rate = audio_info.get("sample_rate", 0)
        if sample_rate >= 32000:
            score += 20
        elif sample_rate >= 22050:
            score += 10
        
        return min(score, 100.0)
    
    def generate_speech(self, text: str, speaker_name: str, language: str = "zh") -> str:
        """
        ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆè¯­éŸ³
        
        Args:
            text: è¦è½¬æ¢çš„æ–‡å­—
            speaker_name: è¯´è¯è€…åç§°
            language: è¯­è¨€ä»£ç 
        
        Returns:
            ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        """
        # v2 æ¨¡å‹è¯­è¨€ä»£ç è½¬æ¢ï¼ˆzh-cn -> zhï¼‰
        language_mapping = {
            "zh-cn": "zh",
            "zh-tw": "zh",
            "en-us": "en",
            "en-gb": "en",
            "ja-jp": "ja",
            "ko-kr": "ko"
        }
        language = language_mapping.get(language.lower(), language)
        
        if speaker_name not in self.speakers_data:
            raise Exception(f"æœªæ‰¾åˆ°è¯´è¯è€… '{speaker_name}' çš„æ•°æ®ï¼Œè¯·å…ˆä¸Šä¼ éŸ³é¢‘è¯­æ–™")
        
        speaker_info = self.speakers_data[speaker_name]
        
        # æ£€æŸ¥æ˜¯å¦å·²è®­ç»ƒ
        if not speaker_info.get("trained", False):
            raise Exception(
                f"è¯´è¯è€… '{speaker_name}' å°šæœªè®­ç»ƒæ¨¡å‹ã€‚"
                f"è¯·å…ˆè®­ç»ƒæ¨¡å‹ï¼ˆå½“å‰æœ‰ {len(speaker_info['audio_files'])} ä¸ªéŸ³é¢‘æ ·æœ¬ï¼‰"
            )
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        output_filename = f"{speaker_name}_{uuid.uuid4().hex[:8]}.wav"
        output_path = os.path.join("outputs", output_filename)
        
        try:
            print(f"ğŸ¤ æ­£åœ¨ä½¿ç”¨è®­ç»ƒæ¨¡å‹ç”Ÿæˆè¯­éŸ³: {text[:50]}...")
            
            model_info = speaker_info.get("model_info", {})
            
            if model_info.get("method") == "gpt_sovits_trained" and self.api_available:
                # ä½¿ç”¨è®­ç»ƒå¥½çš„ GPT-SoVITS æ¨¡å‹é€šè¿‡ API ç”Ÿæˆ
                print(f"   ä½¿ç”¨å®Œæ•´è®­ç»ƒæ¨¡å‹ï¼ˆé«˜è´¨é‡æ¨¡å¼ï¼‰")
                self._generate_with_trained_model(text, model_info, speaker_info, output_path, language)
            elif self.api_available:
                # ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ + å‚è€ƒéŸ³é¢‘é€šè¿‡ API ç”Ÿæˆ
                print(f"   ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ + å‚è€ƒéŸ³é¢‘ï¼ˆè‰¯å¥½è´¨é‡ï¼‰")
                self._generate_with_api_reference(text, speaker_info, output_path, language)
            else:
                # API ä¸å¯ç”¨ï¼Œç”Ÿæˆå ä½éŸ³é¢‘
                print(f"   âš ï¸  API ä¸å¯ç”¨ï¼Œç”Ÿæˆå ä½éŸ³é¢‘")
                self._generate_placeholder(text, speaker_info, output_path, language)
            
            print(f"âœ… éŸ³é¢‘ç”ŸæˆæˆåŠŸ: {output_path}")
            return output_path
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            raise Exception(f"è¯­éŸ³åˆæˆå¤±è´¥: {str(e)}")
    
    def _to_absolute_path(self, path: str) -> str:
        """å°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„"""
        if os.path.isabs(path):
            return path
        return os.path.abspath(path)
    
    def _generate_with_trained_model(self, text: str, model_info: Dict, speaker_info: Dict, output_path: str, language: str):
        """ä½¿ç”¨è®­ç»ƒå¥½çš„ GPT-SoVITS æ¨¡å‹é€šè¿‡ API ç”Ÿæˆè¯­éŸ³ï¼ˆæœ€é«˜è´¨é‡ï¼‰"""
        
        # è·å–æ¨¡å‹è·¯å¾„
        gpt_model = model_info.get("gpt_model")
        sovits_model = model_info.get("sovits_model")
        
        if not gpt_model or not sovits_model:
            raise Exception("æ¨¡å‹è·¯å¾„ä¸å®Œæ•´")
        
        # é€‰æ‹©æœ€ä½³å‚è€ƒéŸ³é¢‘
        best_ref = self._select_best_reference_audio(speaker_info["audio_files"])
        
        # é€‰æ‹©è¾…åŠ©å‚è€ƒéŸ³é¢‘ï¼ˆå¤šéŸ³é¢‘èåˆï¼‰
        aux_refs = self._select_auxiliary_references(speaker_info["audio_files"], count=3)
        
        # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        ref_audio_abs = self._to_absolute_path(best_ref["path"])
        aux_refs_abs = [self._to_absolute_path(a["path"]) for a in aux_refs] if aux_refs else None
        
        print(f"   ğŸ“ å‚è€ƒéŸ³é¢‘ï¼ˆç»å¯¹è·¯å¾„ï¼‰: {ref_audio_abs}")
        
        # è°ƒç”¨ API ç”Ÿæˆ
        success = self.api_client.generate_with_trained_model(
            text=text,
            gpt_model_path=gpt_model,
            sovits_model_path=sovits_model,
            ref_audio_path=ref_audio_abs,
            output_path=output_path,
            text_lang=language,
            prompt_text=best_ref.get("text", ""),
            aux_ref_audio_paths=aux_refs_abs,
            top_k=5,
            top_p=1.0,
            temperature=1.0,
            text_split_method="cut5",
            batch_size=1,
            speed_factor=1.0
        )
        
        if not success:
            raise Exception("API ç”Ÿæˆå¤±è´¥")
    
    def _generate_with_api_reference(self, text: str, speaker_info: Dict, output_path: str, language: str):
        """ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ + å‚è€ƒéŸ³é¢‘é€šè¿‡ API ç”Ÿæˆï¼ˆè‰¯å¥½è´¨é‡ï¼‰"""
        
        # é€‰æ‹©æœ€ä½³å‚è€ƒéŸ³é¢‘
        best_ref = self._select_best_reference_audio(speaker_info["audio_files"])
        
        # é€‰æ‹©è¾…åŠ©å‚è€ƒéŸ³é¢‘
        aux_refs = self._select_auxiliary_references(speaker_info["audio_files"], count=5)
        
        # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        ref_audio_abs = self._to_absolute_path(best_ref["path"])
        aux_refs_abs = [self._to_absolute_path(a["path"]) for a in aux_refs] if aux_refs else None
        
        # è°ƒç”¨ API ç”Ÿæˆ
        success = self.api_client.generate_speech(
            text=text,
            ref_audio_path=ref_audio_abs,
            output_path=output_path,
            text_lang=language,
            prompt_text=best_ref.get("text", ""),
            prompt_lang=language,
            aux_ref_audio_paths=aux_refs_abs,
            top_k=5,
            top_p=1.0,
            temperature=1.0,
            text_split_method="cut5"
        )
        
        if not success:
            raise Exception("API ç”Ÿæˆå¤±è´¥")
    
    def _generate_placeholder(self, text: str, speaker_info: Dict, output_path: str, language: str):
        """ç”Ÿæˆå ä½éŸ³é¢‘ï¼ˆå½“ API ä¸å¯ç”¨æ—¶ï¼‰"""
        print(f"âš ï¸  GPT-SoVITS API ä¸å¯ç”¨ï¼Œç”Ÿæˆå ä½éŸ³é¢‘")
        print(f"   è¯·å¯åŠ¨ API æœåŠ¡: cd {self.sovits_path} && python api_v2.py -p 9880")
        
        # æ ¹æ®æ–‡æœ¬é•¿åº¦ä¼°ç®—æ—¶é•¿
        if language.startswith('zh'):
            estimated_duration = len(text) * 0.3
        else:
            estimated_duration = len(text.split()) * 0.2
        
        duration = max(1.0, min(estimated_duration, 30.0))
        sr = 32000
        
        # åˆ›å»ºé™éŸ³å ä½éŸ³é¢‘
        audio = np.zeros(int(duration * sr))
        sf.write(output_path, audio, sr)
    
    def _generate_with_reference(self, text: str, speaker_info: Dict, output_path: str, language: str):
        """ä½¿ç”¨æ™ºèƒ½å¤šéŸ³é¢‘å‚è€ƒç”Ÿæˆï¼ˆå¢å¼ºç‰ˆï¼‰"""
        audio_files = speaker_info["audio_files"]
        
        if not audio_files:
            raise Exception("æ²¡æœ‰å¯ç”¨çš„å‚è€ƒéŸ³é¢‘")
        
        print(f"ğŸ¯ æ™ºèƒ½éŸ³é¢‘é€‰æ‹©:")
        print(f"   - å¯ç”¨éŸ³é¢‘: {len(audio_files)} ä¸ª")
        
        # æ™ºèƒ½é€‰æ‹©æœ€ä½³å‚è€ƒéŸ³é¢‘
        best_audio = self._select_best_reference_audio(audio_files, text)
        reference_path = best_audio["path"]
        
        print(f"   - âœ… å·²é€‰æ‹©: {os.path.basename(reference_path)}")
        print(f"   - æ—¶é•¿: {best_audio.get('duration', 0):.1f} ç§’")
        print(f"   - è´¨é‡è¯„åˆ†: {self._calculate_audio_quality_score(best_audio):.0f}/100")
        print(f"")
        
        # è¿™é‡Œå¯ä»¥é›†æˆ XTTS æˆ– GPT-SoVITS API
        # å½“å‰ä¸ºæ¼”ç¤ºæ¨¡å¼ï¼Œç”ŸæˆåŸºäºæ–‡æœ¬é•¿åº¦çš„é™éŸ³æ–‡ä»¶
        print(f"ğŸ’¡ å½“å‰æ¨¡å¼: æ¼”ç¤ºæ¨¡å¼ï¼ˆç”Ÿæˆå ä½éŸ³é¢‘ï¼‰")
        print(f"ğŸ”§ è¦ç”ŸæˆçœŸå®è¯­éŸ³ï¼Œå»ºè®®ï¼š")
        print(f"   1. é›†æˆ XTTS å¼•æ“ï¼ˆåœ¨ app.py ä¸­è®¾ç½® USE_SOVITS = Falseï¼‰")
        print(f"   2. æˆ–é€šè¿‡ GPT-SoVITS WebUI è®­ç»ƒå®Œæ•´æ¨¡å‹")
        print(f"")
        
        # ç”Ÿæˆå ä½éŸ³é¢‘ï¼ˆæ ¹æ®æ–‡æœ¬é•¿åº¦ä¼°ç®—ï¼‰
        # ä¸­æ–‡ï¼šå¹³å‡æ¯å­— 0.3 ç§’ï¼Œè‹±æ–‡ï¼šå¹³å‡æ¯è¯ 0.2 ç§’
        if language.startswith('zh'):
            estimated_duration = len(text) * 0.3
        else:
            estimated_duration = len(text.split()) * 0.2
        
        duration = max(1.0, min(estimated_duration, 30.0))  # 1-30ç§’
        sr = 32000
        
        # åˆ›å»ºå ä½éŸ³é¢‘ï¼ˆé™éŸ³ï¼‰
        audio = np.zeros(int(duration * sr))
        sf.write(output_path, audio, sr)
        
        print(f"âœ… å·²ç”Ÿæˆ {duration:.1f} ç§’å ä½éŸ³é¢‘")
        print(f"âš ï¸  æ³¨æ„: è¿™æ˜¯æ¼”ç¤ºç”¨çš„é™éŸ³æ–‡ä»¶ï¼Œä¸æ˜¯çœŸå®è¯­éŸ³")
    
    def _select_best_reference_audio(self, audio_files: list, text: str = "") -> Dict:
        """æ™ºèƒ½é€‰æ‹©æœ€ä½³å‚è€ƒéŸ³é¢‘ï¼ˆéœ€è¦ 3-10 ç§’æ—¶é•¿ï¼‰"""
        if not audio_files:
            raise Exception("æ²¡æœ‰å¯ç”¨çš„éŸ³é¢‘æ–‡ä»¶")
        
        # è¿‡æ»¤æ—¶é•¿ä¸åˆé€‚çš„éŸ³é¢‘ï¼ˆAPI è¦æ±‚ 3-10 ç§’ï¼‰
        valid_audios = []
        for audio in audio_files:
            duration = audio.get("duration", 0)
            if 3.0 <= duration <= 10.0:
                valid_audios.append(audio)
        
        # å¦‚æœæ²¡æœ‰ç¬¦åˆæ—¶é•¿çš„éŸ³é¢‘ï¼ŒæŠ›å‡ºé”™è¯¯
        if not valid_audios:
            durations = [f"{a.get('duration', 0):.1f}s" for a in audio_files]
            raise Exception(
                f"æ‰€æœ‰éŸ³é¢‘éƒ½ä¸ç¬¦åˆ 3-10 ç§’çš„æ—¶é•¿è¦æ±‚ï¼\n"
                f"å½“å‰éŸ³é¢‘æ—¶é•¿: {', '.join(durations)}\n"
                f"è¯·ä¸Šä¼  3-10 ç§’çš„éŸ³é¢‘æ ·æœ¬"
            )
        
        # è®¡ç®—æ¯ä¸ªéŸ³é¢‘çš„ç»¼åˆè¯„åˆ†
        scored_audios = []
        for audio in valid_audios:
            score = self._calculate_audio_quality_score(audio)
            scored_audios.append((score, audio))
        
        # æŒ‰è¯„åˆ†æ’åºï¼Œé€‰æ‹©æœ€é«˜åˆ†çš„
        scored_audios.sort(reverse=True, key=lambda x: x[0])
        
        print(f"   â„¹ï¸  å·²é€‰æ‹©å‚è€ƒéŸ³é¢‘: æ—¶é•¿ {scored_audios[0][1].get('duration', 0):.1f}s, è¯„åˆ† {scored_audios[0][0]:.0f}/100")
        
        # è¿”å›è¯„åˆ†æœ€é«˜çš„éŸ³é¢‘
        return scored_audios[0][1]
    
    def _select_auxiliary_references(self, audio_files: list, count: int = 5) -> List[Dict]:
        """é€‰æ‹©è¾…åŠ©å‚è€ƒéŸ³é¢‘ï¼ˆç”¨äºå¤šéŸ³é¢‘èåˆï¼Œéœ€è¦ 3-10 ç§’æ—¶é•¿ï¼‰"""
        if not audio_files or len(audio_files) <= 1:
            return []
        
        # è¿‡æ»¤æ—¶é•¿ä¸åˆé€‚çš„éŸ³é¢‘ï¼ˆAPI è¦æ±‚ 3-10 ç§’ï¼‰
        valid_audios = []
        for audio in audio_files:
            duration = audio.get("duration", 0)
            if 3.0 <= duration <= 10.0:
                valid_audios.append(audio)
        
        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„æœ‰æ•ˆéŸ³é¢‘ï¼Œè¿”å›ç©ºåˆ—è¡¨
        if len(valid_audios) <= 1:
            return []
        
        # è®¡ç®—æ‰€æœ‰éŸ³é¢‘çš„è¯„åˆ†
        scored_audios = []
        for audio in valid_audios:
            score = self._calculate_audio_quality_score(audio)
            scored_audios.append((score, audio))
        
        # æŒ‰è¯„åˆ†æ’åº
        scored_audios.sort(reverse=True, key=lambda x: x[0])
        
        # è¿”å›å‰ N ä¸ªï¼ˆæ’é™¤ç¬¬ä¸€ä¸ªï¼Œå› ä¸ºå®ƒå·²ç»æ˜¯ä¸»å‚è€ƒï¼‰
        aux_count = min(count, len(scored_audios) - 1)
        return [audio for _, audio in scored_audios[1:aux_count+1]]
    
    def get_training_status(self, speaker_name: str) -> Dict:
        """è·å–è®­ç»ƒçŠ¶æ€"""
        return self.training_status.get(speaker_name, {"status": "not_started"})
    
    def list_speakers(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰è¯´è¯è€…åŠå…¶çŠ¶æ€"""
        speakers = []
        for name, info in self.speakers_data.items():
            speakers.append({
                "name": name,
                "audio_count": len(info["audio_files"]),
                "trained": info.get("trained", False),
                "created_at": info.get("created_at", "æœªçŸ¥"),
                "trained_at": info.get("trained_at", None),
                "model_path": info.get("model_path", None)
            })
        return speakers
    
    def delete_speaker(self, speaker_name: str):
        """åˆ é™¤è¯´è¯è€…æ•°æ®å’Œè®­ç»ƒæ¨¡å‹"""
        if speaker_name not in self.speakers_data:
            raise Exception(f"æœªæ‰¾åˆ°è¯´è¯è€… '{speaker_name}'")
        
        speaker_info = self.speakers_data[speaker_name]
        
        # åˆ é™¤è®­ç»ƒæ•°æ®ç›®å½•
        speaker_dir = os.path.join(self.training_data_dir, speaker_name)
        if os.path.exists(speaker_dir):
            shutil.rmtree(speaker_dir)
        
        # åˆ é™¤è®­ç»ƒæ¨¡å‹
        model_dir = os.path.join(self.trained_models_dir, speaker_name)
        if os.path.exists(model_dir):
            shutil.rmtree(model_dir)
        
        # åˆ é™¤ä¸Šä¼ çš„åŸå§‹æ–‡ä»¶
        for audio_info in speaker_info["audio_files"]:
            original_path = audio_info.get("original_path")
            if original_path and os.path.exists(original_path):
                os.remove(original_path)
        
        # ä»æ•°æ®ä¸­ç§»é™¤
        del self.speakers_data[speaker_name]
        self._save_speakers_data()
        
        print(f"âœ… å·²åˆ é™¤è¯´è¯è€… '{speaker_name}' çš„æ‰€æœ‰æ•°æ®")


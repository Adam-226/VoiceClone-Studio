#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPT-SoVITS è®­ç»ƒç®¡ç†å™¨
è´Ÿè´£å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼šæ•°æ®é¢„å¤„ç† â†’ Stage 1 è®­ç»ƒ â†’ Stage 2 è®­ç»ƒ
"""

import os
import sys
import json
import yaml
import subprocess
import shutil
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import time


class GPTSoVITSTrainer:
    """GPT-SoVITS å®Œæ•´è®­ç»ƒç®¡ç†å™¨"""
    
    def __init__(self, sovits_path: str = None):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            sovits_path: GPT-SoVITS é¡¹ç›®è·¯å¾„
        """
        self.base_dir = os.path.dirname(os.path.abspath(__file__))
        
        # GPT-SoVITS è·¯å¾„
        if sovits_path is None:
            self.sovits_path = os.path.join(self.base_dir, "GPT-SoVITS-main")
        else:
            self.sovits_path = sovits_path
            
        if not os.path.exists(self.sovits_path):
            raise FileNotFoundError(f"GPT-SoVITS è·¯å¾„ä¸å­˜åœ¨: {self.sovits_path}")
        
        # Python è§£é‡Šå™¨
        self.python_exec = sys.executable
        
        # è®­ç»ƒè¾“å‡ºæ ¹ç›®å½•
        self.exp_root = os.path.join(self.base_dir, "training_experiments")
        os.makedirs(self.exp_root, exist_ok=True)
        
        # é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
        self.pretrained_models = {
            "bert": os.path.join(self.sovits_path, "GPT_SoVITS/pretrained_models/chinese-roberta-wwm-ext-large"),
            "ssl": os.path.join(self.sovits_path, "GPT_SoVITS/pretrained_models/chinese-hubert-base"),
            # v2 ç‰ˆæœ¬æ¨¡å‹è·¯å¾„
            "s1_v2": os.path.join(self.sovits_path, "GPT_SoVITS/pretrained_models/gsv-v2final-pretrained/s1bert25hz-5kh-longer-epoch=12-step=369668.ckpt"),
            "s2G_v2": os.path.join(self.sovits_path, "GPT_SoVITS/pretrained_models/gsv-v2final-pretrained/s2G2333k.pth"),
            "s2D_v2": os.path.join(self.sovits_path, "GPT_SoVITS/pretrained_models/gsv-v2final-pretrained/s2D2333k.pth"),
            "s2_config": os.path.join(self.sovits_path, "GPT_SoVITS/configs/s2.json"),
        }
        
        print(f"âœ… GPT-SoVITS è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ GPT-SoVITS è·¯å¾„: {self.sovits_path}")
        print(f"ğŸ“ å®éªŒè¾“å‡ºè·¯å¾„: {self.exp_root}")
    
    def prepare_training_data(
        self, 
        speaker_name: str, 
        audio_files: List[Dict],
        audio_text_map: Optional[Dict[str, str]] = None
    ) -> str:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®
        
        Args:
            speaker_name: è¯´è¯è€…åç§°
            audio_files: éŸ³é¢‘æ–‡ä»¶åˆ—è¡¨ [{"path": "xxx.wav", "duration": 10.5}, ...]
            audio_text_map: éŸ³é¢‘æ–‡ä»¶åˆ°æ–‡æœ¬çš„æ˜ å°„ {"xxx.wav": "è¿™æ˜¯æ–‡æœ¬å†…å®¹"}
            
        Returns:
            å®éªŒç›®å½•è·¯å¾„
        """
        print(f"\nğŸ¯ å¼€å§‹å‡†å¤‡è®­ç»ƒæ•°æ®: {speaker_name}")
        print(f"   éŸ³é¢‘æ–‡ä»¶æ•°é‡: {len(audio_files)}")
        
        # åˆ›å»ºå®éªŒç›®å½•
        exp_dir = os.path.join(self.exp_root, speaker_name)
        os.makedirs(exp_dir, exist_ok=True)
        
        # åˆ›å»ºè¾“å…¥ç›®å½•
        input_wav_dir = os.path.join(exp_dir, "input_wavs")
        os.makedirs(input_wav_dir, exist_ok=True)
        
        # å¤åˆ¶éŸ³é¢‘æ–‡ä»¶
        print(f"ğŸ“‹ å¤åˆ¶éŸ³é¢‘æ–‡ä»¶åˆ°: {input_wav_dir}")
        copied_files = []  # è®°å½•å®é™…å¤åˆ¶çš„æ–‡ä»¶å
        for i, audio_info in enumerate(audio_files):
            src_path = audio_info["path"]
            if not os.path.exists(src_path):
                print(f"   âš ï¸  è·³è¿‡ä¸å­˜åœ¨çš„æ–‡ä»¶: {src_path}")
                continue
                
            # ä½¿ç”¨ç»Ÿä¸€çš„å‘½åæ ¼å¼
            ext = os.path.splitext(src_path)[1]
            dst_filename = f"{speaker_name}_{i:04d}{ext}"
            dst_path = os.path.join(input_wav_dir, dst_filename)
            
            shutil.copy2(src_path, dst_path)
            copied_files.append(dst_filename)  # ä¿å­˜å®é™…æ–‡ä»¶å
            print(f"   âœ… {i+1}/{len(audio_files)}: {dst_filename}")
        
        # åˆ›å»ºæ–‡æœ¬æ ‡æ³¨æ–‡ä»¶
        text_file = os.path.join(exp_dir, "input_text.txt")
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æœ‰æ•ˆçš„æ–‡æœ¬æ ‡æ³¨æ–‡ä»¶
        if os.path.exists(text_file):
            # æ£€æŸ¥æ–‡ä»¶å†…å®¹æ˜¯å¦æœ‰æ•ˆï¼ˆä¸æ˜¯å ä½æ–‡æœ¬ï¼‰
            with open(text_file, "r", encoding="utf-8") as f:
                first_line = f.readline()
                if "è¿™æ˜¯ä¸€æ®µè®­ç»ƒè¯­éŸ³" not in first_line and len(first_line.strip()) > 50:
                    print(f"ğŸ“ ä½¿ç”¨å·²å­˜åœ¨çš„æ–‡æœ¬æ ‡æ³¨æ–‡ä»¶")
                    print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {exp_dir}")
                    return exp_dir
        
        if audio_text_map:
            # ä½¿ç”¨æä¾›çš„æ–‡æœ¬æ˜ å°„
            print(f"ğŸ“ ä½¿ç”¨æä¾›çš„æ–‡æœ¬æ ‡æ³¨")
            with open(text_file, "w", encoding="utf-8") as f:
                for i, (audio_info, dst_filename) in enumerate(zip(audio_files, copied_files)):
                    basename = os.path.basename(audio_info["path"])
                    text = audio_text_map.get(basename, "è¿™æ˜¯ä¸€æ®µè¯­éŸ³ã€‚")
                    # æ ¼å¼: æ–‡ä»¶å|è¯´è¯è€…|è¯­è¨€|æ–‡æœ¬
                    f.write(f"{input_wav_dir}/{dst_filename}|{speaker_name}|ZH|{text}\n")
        else:
            # ä½¿ç”¨ ASR è‡ªåŠ¨è¯†åˆ«æ–‡æœ¬
            print(f"ğŸ“ ä½¿ç”¨ ASR è‡ªåŠ¨è¯†åˆ«éŸ³é¢‘æ–‡æœ¬...")
            asr_success = False
            
            try:
                # åŠ¨æ€å¯¼å…¥ ASR æ¨¡å—
                import sys
                # å°† GPT-SoVITS æ ¹ç›®å½•åŠ å…¥ sys.pathï¼ˆä¸æ˜¯ tools/asr å­ç›®å½•ï¼ï¼‰
                if self.sovits_path not in sys.path:
                    sys.path.insert(0, self.sovits_path)
                
                # å¯¼å…¥ FasterWhisper ASRï¼ˆç°åœ¨ä» tools.asr æ¨¡å—å¯¼å…¥ï¼‰
                from tools.asr.fasterwhisper_asr import execute_asr
                
                # è°ƒç”¨ ASRï¼ˆä¼šè‡ªåŠ¨ç”Ÿæˆ {speaker_name}.list æ–‡ä»¶ï¼‰
                print(f"   ğŸ¤ æ­£åœ¨è¯†åˆ«éŸ³é¢‘å†…å®¹ï¼ˆå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
                # ä½¿ç”¨ç»å¯¹è·¯å¾„æˆ–æ¨¡å‹åç§°
                model_path = os.path.join(self.sovits_path, "tools", "asr", "models", "faster-whisper-large-v3")
                if not os.path.exists(model_path):
                    # å¦‚æœæœ¬åœ°è·¯å¾„ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ¨¡å‹åç§°è®© ASR è‡ªåŠ¨ä¸‹è½½
                    model_path = "large-v3"
                    print(f"   ğŸ“¥ æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°†è‡ªåŠ¨ä¸‹è½½ faster-whisper-{model_path}...")
                
                asr_output = execute_asr(
                    input_folder=input_wav_dir,
                    output_folder=exp_dir,
                    model_path=model_path,
                    language="zh",  # ä¸­æ–‡
                    precision="float16"
                )
                
                # å°† ASR è¾“å‡ºå¤åˆ¶ä¸º input_text.txt
                if os.path.exists(asr_output):
                    shutil.copy(asr_output, text_file)
                    print(f"   âœ… ASR è¯†åˆ«å®Œæˆï¼Œå·²ç”Ÿæˆæ–‡æœ¬æ ‡æ³¨")
                    asr_success = True
                else:
                    print(f"   âš ï¸  ASR è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨: {asr_output}")
                    
            except Exception as e:
                print(f"   âš ï¸  ASR è¯†åˆ«å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
            
            # å¦‚æœ ASR å¤±è´¥ï¼Œä½¿ç”¨å ä½æ–‡æœ¬ï¼ˆä¼šå¯¼è‡´è®­ç»ƒå¤±è´¥ï¼‰
            if not asr_success:
                print(f"   âŒ å›é€€åˆ°å ä½æ–‡æœ¬ï¼ˆè®­ç»ƒå°†å¤±è´¥ï¼Œè¯·å®‰è£… ASR ä¾èµ–æˆ–æ‰‹åŠ¨æä¾›æ–‡æœ¬ï¼‰")
                with open(text_file, "w", encoding="utf-8") as f:
                    for dst_filename in copied_files:
                        f.write(f"{input_wav_dir}/{dst_filename}|{speaker_name}|ZH|è¿™æ˜¯ä¸€æ®µè®­ç»ƒè¯­éŸ³ã€‚\n")
        
        print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {exp_dir}")
        return exp_dir
    
    def run_data_preprocessing(self, exp_dir: str, speaker_name: str) -> bool:
        """
        è¿è¡Œæ•°æ®é¢„å¤„ç†ï¼ˆæ­¥éª¤ 1a, 1b, 1cï¼‰
        
        Args:
            exp_dir: å®éªŒç›®å½•
            speaker_name: è¯´è¯è€…åç§°
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        print(f"\nğŸ”§ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
        
        # æ¸…ç†ä¹‹å‰çš„é¢„å¤„ç†è¾“å‡ºï¼ˆé¿å…è„šæœ¬è·³è¿‡å·²å­˜åœ¨ä½†ä¸å®Œæ•´çš„æ–‡ä»¶ï¼‰
        for subdir in ["2-name2text", "3-bert", "4-cnhubert", "5-wav32k", "6-name2semantic"]:
            dir_path = os.path.join(exp_dir, subdir)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)
                print(f"   ğŸ§¹ æ¸…ç†æ—§æ•°æ®: {subdir}")
        
        # æ¸…ç†æ—§çš„è®­ç»ƒ checkpointï¼ˆé¿å… epoch å†²çªï¼‰
        for checkpoint_dir in ["logs_s1/ckpt", "logs_s2_v2"]:
            dir_path = os.path.join(exp_dir, checkpoint_dir)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)
                print(f"   ğŸ§¹ æ¸…ç†æ—§ checkpoint: {checkpoint_dir}")
        
        # åˆ é™¤æ—§çš„ tsv/txt æ–‡ä»¶
        for old_file in ["2-name2text-0.txt", "6-name2semantic-0.tsv"]:
            file_path = os.path.join(exp_dir, old_file)
            if os.path.exists(file_path):
                os.remove(file_path)
                print(f"   ğŸ§¹ æ¸…ç†æ—§æ–‡ä»¶: {old_file}")
        
        input_wav_dir = os.path.join(exp_dir, "input_wavs")
        input_text = os.path.join(exp_dir, "input_text.txt")
        
        if not os.path.exists(input_text):
            print(f"âŒ æ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {input_text}")
            return False
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        env.update({
            "inp_text": input_text,
            "inp_wav_dir": input_wav_dir,
            "exp_name": speaker_name,
            "opt_dir": exp_dir,
            "i_part": "0",
            "all_parts": "1",
            "bert_pretrained_dir": self.pretrained_models["bert"],
            "cnhubert_base_dir": self.pretrained_models["ssl"],  # æ­¥éª¤ 1b éœ€è¦
            "pretrained_s2G": self.pretrained_models["s2G_v2"],   # æ­¥éª¤ 1c éœ€è¦
            "s2config_path": self.pretrained_models["s2_config"], # æ­¥éª¤ 1c éœ€è¦
            "is_half": "True",
            # æ·»åŠ  PYTHONPATH ä»¥ä¾¿å¯¼å…¥ GPT-SoVITS çš„å†…éƒ¨æ¨¡å—
            "PYTHONPATH": self.sovits_path + (f":{env.get('PYTHONPATH', '')}" if env.get('PYTHONPATH') else ""),
        })
        
        # æ­¥éª¤ 1a: æ–‡æœ¬å¤„ç†å’Œ BERT ç‰¹å¾æå–
        print(f"   [1/3] æ–‡æœ¬å¤„ç†å’Œ BERT ç‰¹å¾æå–...")
        script_1a = os.path.join(self.sovits_path, "GPT_SoVITS/prepare_datasets/1-get-text.py")
        
        # ä½¿ç”¨åŒ…è£…è„šæœ¬æ¥æ­£ç¡®è®¾ç½®ç¯å¢ƒ
        wrapper_script = os.path.join(self.base_dir, "run_preprocessing_script.py")
        cmd_1a = [self.python_exec, wrapper_script, self.sovits_path, script_1a]
        
        result = subprocess.run(cmd_1a, env=env, capture_output=True, text=True)
        
        # æ˜¾ç¤ºè¾“å‡ºï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if result.stdout:
            print(f"   ğŸ“‹ æ­¥éª¤ 1a è¾“å‡º:")
            for line in result.stdout.strip().split('\n')[:10]:  # æ˜¾ç¤ºå‰10è¡Œ
                print(f"      {line}")
        
        if result.returncode != 0:
            print(f"   âŒ æ­¥éª¤ 1a å¤±è´¥:")
            print(result.stderr)
            return False
        
        # éªŒè¯è¾“å‡ºæ–‡ä»¶
        bert_dir = os.path.join(exp_dir, "2-name2text")
        if os.path.exists(bert_dir):
            bert_files = [f for f in os.listdir(bert_dir) if f.endswith('.bert.pt')]
            print(f"   âœ… æ­¥éª¤ 1a å®Œæˆ ({len(bert_files)} .bert.pt)")
        else:
            print(f"   âœ… æ­¥éª¤ 1a å®Œæˆ")
        
        # æ­¥éª¤ 1b: SSL ç‰¹å¾æå–
        print(f"   [2/3] SSL ç‰¹å¾æå–ï¼ˆHuBERTï¼‰...")
        script_1b = os.path.join(self.sovits_path, "GPT_SoVITS/prepare_datasets/2-get-hubert-wav32k.py")
        
        # ä½¿ç”¨åŒ…è£…è„šæœ¬æ¥æ­£ç¡®è®¾ç½®ç¯å¢ƒ
        wrapper_script = os.path.join(self.base_dir, "run_preprocessing_script.py")
        cmd_1b = [self.python_exec, wrapper_script, self.sovits_path, script_1b]
        
        result = subprocess.run(cmd_1b, env=env, capture_output=True, text=True)
        
        # æ˜¾ç¤ºè¾“å‡ºï¼ˆå³ä½¿æˆåŠŸä¹Ÿæ˜¾ç¤ºï¼Œç”¨äºè°ƒè¯•ï¼‰
        if result.stdout:
            print(f"   ğŸ“‹ æ­¥éª¤ 1b è¾“å‡º:")
            for line in result.stdout.strip().split('\n')[:20]:  # æ˜¾ç¤ºå‰20è¡Œ
                print(f"      {line}")
        
        if result.returncode != 0:
            print(f"   âŒ æ­¥éª¤ 1b å¤±è´¥:")
            if result.stderr:
                print(result.stderr)
            return False
        
        # éªŒè¯è¾“å‡ºæ–‡ä»¶
        hubert_dir = os.path.join(exp_dir, "4-cnhubert")
        wav32_dir = os.path.join(exp_dir, "5-wav32k")
        hubert_files = [f for f in os.listdir(hubert_dir) if f.endswith('.pt')] if os.path.exists(hubert_dir) else []
        # æ³¨æ„ï¼šwav32k ç›®å½•ä¸­çš„æ–‡ä»¶å¯èƒ½æœ‰å„ç§æ‰©å±•åï¼ˆ.mp3, .wav ç­‰ï¼‰ï¼Œæ‰€ä»¥ç»Ÿè®¡æ‰€æœ‰æ–‡ä»¶
        wav32_files = os.listdir(wav32_dir) if os.path.exists(wav32_dir) else []
        
        print(f"   âœ… æ­¥éª¤ 1b å®Œæˆ ({len(hubert_files)} .pt, {len(wav32_files)} éŸ³é¢‘æ–‡ä»¶)")
        
        # æ­¥éª¤ 1c: è¯­ä¹‰ç‰¹å¾æå–
        print(f"   [3/3] è¯­ä¹‰ç‰¹å¾æå–...")
        
        # å…ˆæ£€æŸ¥æ­¥éª¤ 1b çš„è¾“å‡ºå®Œæ•´æ€§
        print(f"   ğŸ” æ£€æŸ¥æ­¥éª¤ 1b è¾“å‡º:")
        input_text_file = os.path.join(exp_dir, "input_text.txt")
        with open(input_text_file, "r", encoding="utf-8") as f:
            expected_files = []
            for line in f.read().strip().split('\n'):
                if line:
                    wav_name = os.path.basename(line.split('|')[0])
                    expected_files.append(wav_name)
        
        missing_pt = []
        for wav_name in expected_files:
            pt_file = os.path.join(exp_dir, "4-cnhubert", f"{wav_name}.pt")
            if not os.path.exists(pt_file):
                missing_pt.append(wav_name)
        
        if missing_pt:
            print(f"   âš ï¸  è­¦å‘Š: {len(missing_pt)} ä¸ªæ–‡ä»¶ç¼ºå°‘ .pt ç‰¹å¾æ–‡ä»¶:")
            for fname in missing_pt[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"      - {fname}")
        else:
            print(f"   âœ… æ‰€æœ‰ {len(expected_files)} ä¸ªæ–‡ä»¶éƒ½æœ‰ .pt ç‰¹å¾")
        
        script_1c = os.path.join(self.sovits_path, "GPT_SoVITS/prepare_datasets/3-get-semantic.py")
        
        # ä½¿ç”¨åŒ…è£…è„šæœ¬æ¥æ­£ç¡®è®¾ç½®ç¯å¢ƒ
        wrapper_script = os.path.join(self.base_dir, "run_preprocessing_script.py")
        cmd_1c = [self.python_exec, wrapper_script, self.sovits_path, script_1c]
        
        result = subprocess.run(cmd_1c, env=env, capture_output=True, text=True)
        if result.returncode != 0:
            print(f"   âŒ æ­¥éª¤ 1c å¤±è´¥:")
            print(result.stderr)
            return False
        
        # è¾“å‡ºæ ‡å‡†è¾“å‡ºä»¥ä¾¿è°ƒè¯•
        if result.stdout:
            print(f"   ğŸ“‹ æ­¥éª¤ 1c è¾“å‡º:")
            for line in result.stdout.strip().split('\n')[:10]:  # åªæ˜¾ç¤ºå‰10è¡Œ
                print(f"      {line}")
        
        # éªŒè¯è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”éç©º
        semantic_file = os.path.join(exp_dir, "6-name2semantic-0.tsv")
        
        # å…³é”®ä¿®å¤ï¼šæ£€æŸ¥å¹¶ä¿®æ­£ semantic æ–‡ä»¶æ ¼å¼
        # GPT-SoVITS çš„ dataset.py æœŸæœ›æ–‡ä»¶æ²¡æœ‰ headerï¼Œä½† pandas é»˜è®¤ä¼šæŠŠç¬¬ä¸€è¡Œå½“ä½œ header
        # æˆ‘ä»¬ä¸æ·»åŠ åˆ—åï¼Œè€Œæ˜¯ç¡®ä¿ GPT-SoVITS æ­£ç¡®è¯»å–æ•°æ®
        if not os.path.exists(semantic_file):
            print(f"   âŒ æ­¥éª¤ 1c å¤±è´¥: è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨: {semantic_file}")
            return False
        
        # æ£€æŸ¥ç”Ÿæˆçš„ semantic æ•°æ®è¡Œæ•°ï¼ˆä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„æ–¹å¼åŠ è½½ï¼‰
        import pandas as pd
        try:
            semantic_df = pd.read_csv(semantic_file, delimiter="\t", encoding="utf-8", header=None)
            semantic_count = len(semantic_df)
        except Exception as e:
            print(f"   âš ï¸  è­¦å‘Š: æ— æ³•è¯»å– semantic æ–‡ä»¶: {e}")
            semantic_count = 0
        
        print(f"   ğŸ“Š ç”Ÿæˆäº† {semantic_count} æ¡æœ‰æ•ˆ semantic æ•°æ®ï¼ˆæœŸæœ› {len(expected_files)} æ¡ï¼‰")
        
        if semantic_count != len(expected_files):
            print(f"   âš ï¸  è­¦å‘Š: semantic æ•°æ®æ•°é‡ä¸é¢„æœŸä¸ç¬¦ï¼")
            print(f"      å·®å¼‚: {len(expected_files) - semantic_count} æ¡æ•°æ®ç¼ºå¤±æˆ–æ— æ•ˆ")
            
            # æ˜¾ç¤ºå‰å‡ è¡Œæ•°æ®ä»¥ä¾¿è¯Šæ–­
            print(f"   ğŸ” æ£€æŸ¥æ–‡ä»¶å†…å®¹:")
            with open(semantic_file, "r", encoding="utf-8") as f:
                lines = f.readlines()
                print(f"      æ€»è¡Œæ•°ï¼ˆåŒ…å«æ ‡é¢˜/ç©ºè¡Œï¼‰: {len(lines)}")
                if len(lines) > 0:
                    print(f"      ç¬¬ä¸€è¡Œ: {lines[0][:100] if len(lines[0]) > 100 else lines[0].strip()}")
                if len(lines) > 1:
                    print(f"      ç¬¬äºŒè¡Œ: {lines[1][:100] if len(lines[1]) > 100 else lines[1].strip()}")
                if len(lines) > len(expected_files):
                    print(f"      æœ€åä¸€è¡Œ: {lines[-1][:100] if len(lines[-1]) > 100 else lines[-1].strip()}")
        
        if os.path.getsize(semantic_file) == 0:
            print(f"   âŒ æ­¥éª¤ 1c å¤±è´¥: è¾“å‡ºæ–‡ä»¶ä¸ºç©º: {semantic_file}")
            print(f"   ğŸ’¡ æç¤º: æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„éŸ³é¢‘æ ·æœ¬å’Œæ­£ç¡®çš„æ–‡æœ¬æ ‡æ³¨")
            
            # è¯Šæ–­ä¿¡æ¯
            print(f"\n   ğŸ” è¯Šæ–­ä¿¡æ¯:")
            
            # æ£€æŸ¥ input_text.txt
            input_text_file = os.path.join(exp_dir, "input_text.txt")
            if os.path.exists(input_text_file):
                with open(input_text_file, "r", encoding="utf-8") as f:
                    lines = f.read().strip().split('\n')
                    print(f"   ğŸ“„ input_text.txt: {len(lines)} è¡Œ")
                    if lines:
                        print(f"      ç¬¬ä¸€è¡Œ: {lines[0][:100]}")
            else:
                print(f"   âŒ input_text.txt ä¸å­˜åœ¨")
            
            # æ£€æŸ¥ 4-cnhubert ç›®å½•
            hubert_dir = os.path.join(exp_dir, "4-cnhubert")
            if os.path.exists(hubert_dir):
                hubert_files = [f for f in os.listdir(hubert_dir) if f.endswith('.pt')]
                print(f"   ğŸ“ 4-cnhubert: {len(hubert_files)} ä¸ª .pt æ–‡ä»¶")
            else:
                print(f"   âŒ 4-cnhubert ç›®å½•ä¸å­˜åœ¨")
            
            # æ£€æŸ¥ 5-wav32k ç›®å½•
            wav32_dir = os.path.join(exp_dir, "5-wav32k")
            if os.path.exists(wav32_dir):
                wav_files = os.listdir(wav32_dir)
                print(f"   ğŸ“ 5-wav32k: {len(wav_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
            else:
                print(f"   âŒ 5-wav32k ç›®å½•ä¸å­˜åœ¨")
            
            # è¾“å‡ºè„šæœ¬çš„æ ‡å‡†è¾“å‡ºå’Œé”™è¯¯ï¼ˆå³ä½¿ä¸ºç©ºä¹Ÿæ˜¾ç¤ºï¼‰
            print(f"\n   ğŸ“‹ æ­¥éª¤ 1c æ ‡å‡†è¾“å‡º:")
            if result.stdout:
                print(result.stdout)
            else:
                print("      (æ— è¾“å‡º)")
            
            print(f"\n   ğŸ“‹ æ­¥éª¤ 1c é”™è¯¯è¾“å‡º:")
            if result.stderr:
                print(result.stderr)
            else:
                print("      (æ— é”™è¯¯)")
            
            return False
        
        print(f"   âœ… æ­¥éª¤ 1c å®Œæˆ")
        
        print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
        return True
    
    def train_stage1_gpt(
        self, 
        exp_dir: str, 
        speaker_name: str,
        epochs: int = 15,
        batch_size: int = 8,
        save_every_epoch: int = 5
    ) -> Optional[str]:
        """
        è®­ç»ƒ Stage 1: GPT æ¨¡å‹ï¼ˆæ–‡æœ¬åˆ°è¯­ä¹‰ï¼‰
        
        Args:
            exp_dir: å®éªŒç›®å½•
            speaker_name: è¯´è¯è€…åç§°
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            save_every_epoch: æ¯éš”å‡ ä¸ª epoch ä¿å­˜ä¸€æ¬¡
            
        Returns:
            è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœå¤±è´¥è¿”å› None
        """
        print(f"\nğŸ“ å¼€å§‹ Stage 1 è®­ç»ƒï¼ˆGPT æ¨¡å‹ï¼‰...")
        print(f"   Epochs: {epochs}, Batch Size: {batch_size}")
        
        # è®­ç»ƒå‰æ•°æ®éªŒè¯ï¼ˆä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„æ–¹å¼åŠ è½½ï¼‰
        print(f"\n   ğŸ” éªŒè¯è®­ç»ƒæ•°æ®...")
        semantic_file = os.path.join(exp_dir, "6-name2semantic-0.tsv")
        phoneme_file = os.path.join(exp_dir, "2-name2text-0.txt")
        
        # ä½¿ç”¨ pandas åŠ è½½ semantic æ•°æ®ï¼ˆä¸è®­ç»ƒè„šæœ¬ä¸€è‡´ï¼‰
        import pandas as pd
        try:
            semantic_df = pd.read_csv(semantic_file, delimiter="\t", encoding="utf-8", header=None)
            semantic_count = len(semantic_df)
        except Exception as e:
            print(f"   âŒ é”™è¯¯: æ— æ³•åŠ è½½ semantic æ•°æ®: {e}")
            return None
        
        # åŠ è½½ phoneme æ•°æ®
        phoneme_data = {}
        with open(phoneme_file, "r", encoding="utf8") as f:
            lines = f.read().strip("\n").split("\n")
            for line in lines:
                tmp = line.split("\t")
                if len(tmp) == 4:
                    phoneme_data[tmp[0]] = [tmp[1], tmp[2], tmp[3]]
        phoneme_count = len(phoneme_data)
        
        print(f"   ğŸ“Š Semantic æ•°æ®: {semantic_count} æ¡")
        print(f"   ğŸ“Š Phoneme æ•°æ®: {phoneme_count} æ¡")
        
        if semantic_count != phoneme_count:
            print(f"   âŒ é”™è¯¯: æ•°æ®æ•°é‡ä¸åŒ¹é…ï¼")
            print(f"      Semantic: {semantic_count} æ¡")
            print(f"      Phoneme: {phoneme_count} æ¡")
            print(f"      å·®å¼‚: {abs(semantic_count - phoneme_count)} æ¡")
            
            # åˆ—å‡ºä¸åŒ¹é…çš„æ–‡ä»¶ï¼ˆsemantic_df æ²¡æœ‰ headerï¼Œä½¿ç”¨æ•°å­—ç´¢å¼•ï¼‰
            if semantic_count < phoneme_count:
                semantic_names = set(semantic_df.iloc[:, 0].tolist())  # ç¬¬ 0 åˆ—æ˜¯æ–‡ä»¶å
                phoneme_names = set(phoneme_data.keys())
                missing = phoneme_names - semantic_names
                if missing:
                    print(f"      ç¼ºå°‘ semantic æ•°æ®çš„æ–‡ä»¶:")
                    for fname in list(missing)[:5]:
                        print(f"         - {fname}")
            elif semantic_count > phoneme_count:
                semantic_names = set(semantic_df.iloc[:, 0].tolist())  # ç¬¬ 0 åˆ—æ˜¯æ–‡ä»¶å
                phoneme_names = set(phoneme_data.keys())
                extra = semantic_names - phoneme_names
                if extra:
                    print(f"      ç¼ºå°‘ phoneme æ•°æ®çš„æ–‡ä»¶:")
                    for fname in list(extra)[:5]:
                        print(f"         - {fname}")
            
            print(f"\n   ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
            print(f"      1. æ£€æŸ¥å“ªä¸ªæ–‡ä»¶å¯¼è‡´æ•°æ®ä¸åŒ¹é…ï¼ˆè§ä¸Šæ–¹åˆ—è¡¨ï¼‰")
            print(f"      2. åˆ é™¤è¯¥è¯´è¯è€…ï¼Œé‡æ–°ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶")
            print(f"      3. ç¡®ä¿æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶æ ¼å¼æ­£ç¡®ã€æ—¶é•¿åˆé€‚ï¼ˆ1-60ç§’ï¼‰")
            print(f"      4. å¦‚æœé—®é¢˜æŒç»­ï¼ŒæŸ¥çœ‹é¢„å¤„ç†æ—¥å¿—çš„è¯¦ç»†è¾“å‡º")
            
            return None  # æ•°æ®ä¸åŒ¹é…æ—¶åœæ­¢è®­ç»ƒ
        
        if semantic_count == 0:
            print(f"   âŒ é”™è¯¯: Semantic æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒ")
            return None
        
        # åŠ è½½é…ç½®æ¨¡æ¿ï¼ˆä½¿ç”¨ v2 ç‰ˆæœ¬é…ç½®ï¼‰
        config_template = os.path.join(self.sovits_path, "GPT_SoVITS/configs/s1longer-v2.yaml")
        
        with open(config_template, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
        
        # ä¿®æ”¹é…ç½®
        config["train"]["epochs"] = epochs
        config["train"]["batch_size"] = batch_size
        config["train"]["save_every_n_epoch"] = save_every_epoch
        config["train"]["if_save_latest"] = True
        config["train"]["if_save_every_weights"] = True
        config["train"]["half_weights_save_dir"] = os.path.join(exp_dir, "logs_s1")  # åŠç²¾åº¦æƒé‡ä¿å­˜ç›®å½•
        config["train"]["exp_name"] = speaker_name
        config["pretrained_s1"] = self.pretrained_models["s1_v2"]
        
        # è®¾ç½®æ•°æ®è·¯å¾„ï¼ˆæ³¨æ„ï¼šé¢„å¤„ç†è„šæœ¬ç”Ÿæˆçš„æ–‡ä»¶ååŒ…å« -0 åç¼€ï¼‰
        config["train_semantic_path"] = os.path.join(exp_dir, "6-name2semantic-0.tsv")
        config["train_phoneme_path"] = os.path.join(exp_dir, "2-name2text-0.txt")  # æ³¨æ„ï¼šè„šæœ¬ç”Ÿæˆçš„æ–‡ä»¶åæœ‰ -0 åç¼€
        config["output_dir"] = os.path.join(exp_dir, "logs_s1")
        
        # ä¿å­˜ä¸´æ—¶é…ç½®
        temp_config = os.path.join(exp_dir, "s1_config.yaml")
        with open(temp_config, "w", encoding="utf-8") as f:
            yaml.dump(config, f, allow_unicode=True)
        
        # æ‰§è¡Œè®­ç»ƒ
        script_s1 = os.path.join(self.sovits_path, "GPT_SoVITS/s1_train.py")
        cmd = [self.python_exec, script_s1, "--config_file", temp_config]
        
        # è®¾ç½®ç¯å¢ƒå˜é‡å’Œå·¥ä½œç›®å½•
        env = os.environ.copy()
        env["PYTHONPATH"] = self.sovits_path + (f":{env.get('PYTHONPATH', '')}" if env.get('PYTHONPATH') else "")
        
        print(f"   ğŸš€ å¼€å§‹è®­ç»ƒ...")
        print(f"   å‘½ä»¤: {' '.join(cmd)}")
        
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
            env=env,
            cwd=self.sovits_path
        )
        
        # å®æ—¶è¾“å‡ºæ—¥å¿—
        for line in process.stdout:
            print(f"   {line.strip()}")
        
        process.wait()
        
        if process.returncode != 0:
            print(f"   âŒ Stage 1 è®­ç»ƒå¤±è´¥")
            return None
        
        # æŸ¥æ‰¾è®­ç»ƒå¥½çš„æ¨¡å‹
        logs_dir = os.path.join(exp_dir, "logs_s1")
        if not os.path.exists(logs_dir):
            print(f"   âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {logs_dir}")
            return None
        
        # æŸ¥æ‰¾æœ€æ–°çš„ checkpoint
        ckpt_files = [f for f in os.listdir(logs_dir) if f.endswith(".ckpt")]
        if not ckpt_files:
            print(f"   âŒ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹")
            return None
        
        # æŒ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„
        ckpt_files.sort(key=lambda x: os.path.getmtime(os.path.join(logs_dir, x)), reverse=True)
        model_path = os.path.join(logs_dir, ckpt_files[0])
        
        print(f"   âœ… Stage 1 è®­ç»ƒå®Œæˆ")
        print(f"   ğŸ“¦ æ¨¡å‹è·¯å¾„: {model_path}")
        
        return model_path
    
    def _convert_checkpoint_to_weight(
        self, 
        checkpoint_path: str, 
        output_dir: str, 
        speaker_name: str,
        config_path: str
    ) -> Optional[str]:
        """
        å°†è®­ç»ƒ checkpoint è½¬æ¢ä¸ºåŒ…å« config çš„å®Œæ•´æƒé‡æ–‡ä»¶
        
        Args:
            checkpoint_path: checkpoint æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            speaker_name: è¯´è¯è€…åç§°
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            
        Returns:
            è½¬æ¢åçš„æƒé‡æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å› None
        """
        try:
            import torch
            import json
            from collections import OrderedDict
            
            # åŠ è½½ checkpoint
            checkpoint = torch.load(checkpoint_path, map_location="cpu")
            
            # åŠ è½½é…ç½®
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # åˆ›å»ºæƒé‡å­—å…¸
            opt = OrderedDict()
            opt["weight"] = OrderedDict()
            
            # ä» checkpoint æå–æ¨¡å‹æƒé‡
            if "model" in checkpoint:
                state_dict = checkpoint["model"]
            elif "state_dict" in checkpoint:
                state_dict = checkpoint["state_dict"]
            else:
                state_dict = checkpoint
            
            # è¿‡æ»¤å¹¶è½¬æ¢æƒé‡
            for key, value in state_dict.items():
                if "enc_q" in key:
                    continue
                try:
                    opt["weight"][key] = value.half()
                except:
                    opt["weight"][key] = value
            
            # æ·»åŠ é…ç½®ï¼ˆè½¬æ¢ä¸º HParams æ ¼å¼ï¼‰
            class HParams:
                def __init__(self, **kwargs):
                    for k, v in kwargs.items():
                        if isinstance(v, dict):
                            setattr(self, k, HParams(**v))
                        else:
                            setattr(self, k, v)
            
            # å°† config è½¬æ¢ä¸º HParams
            hps = HParams(**config)
            opt["config"] = hps
            
            # æ·»åŠ è®­ç»ƒä¿¡æ¯
            epoch = checkpoint.get("epoch", 8)
            iteration = checkpoint.get("iteration", checkpoint.get("step", 0))
            opt["info"] = f"{epoch}epoch_{iteration}iteration"
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            output_filename = f"{speaker_name}_e{epoch}.pth"
            output_path = os.path.join(output_dir, output_filename)
            
            # ä¿å­˜æƒé‡æ–‡ä»¶
            torch.save(opt, output_path)
            
            print(f"   âœ… è½¬æ¢æˆåŠŸ: {output_filename}")
            return output_path
            
        except Exception as e:
            print(f"   âŒ è½¬æ¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def train_stage2_sovits(
        self,
        exp_dir: str,
        speaker_name: str,
        epochs: int = 10,
        batch_size: int = 8,
        save_every_epoch: int = 4
    ) -> Optional[str]:
        """
        è®­ç»ƒ Stage 2: SoVITS æ¨¡å‹ï¼ˆéŸ³è‰²å…‹éš†ï¼‰
        
        Args:
            exp_dir: å®éªŒç›®å½•
            speaker_name: è¯´è¯è€…åç§°
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            save_every_epoch: æ¯éš”å‡ ä¸ª epoch ä¿å­˜ä¸€æ¬¡
            
        Returns:
            è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœå¤±è´¥è¿”å› None
        """
        print(f"\nğŸ¤ å¼€å§‹ Stage 2 è®­ç»ƒï¼ˆSoVITS æ¨¡å‹ï¼‰...")
        print(f"   Epochs: {epochs}, Batch Size: {batch_size}")
        
        # åŠ è½½é…ç½®æ¨¡æ¿
        config_template = os.path.join(self.sovits_path, "GPT_SoVITS/configs/s2.json")
        
        with open(config_template, "r", encoding="utf-8") as f:
            config = json.load(f)
        
        # ä¿®æ”¹é…ç½®
        config["train"]["epochs"] = epochs
        config["train"]["batch_size"] = batch_size
        config["train"]["save_every_epoch"] = save_every_epoch
        config["train"]["if_save_latest"] = True
        config["train"]["if_save_every_weights"] = True
        config["train"]["name"] = speaker_name
        config["train"]["gpu_numbers"] = "0"  # ä½¿ç”¨ç¬¬ä¸€ä¸ª GPU
        config["train"]["pretrained_s2G"] = self.pretrained_models["s2G_v2"]
        config["train"]["pretrained_s2D"] = self.pretrained_models["s2D_v2"]
        
        # è®¾ç½®é¡¶å±‚ nameï¼ˆs2_train.py éœ€è¦ï¼‰
        config["name"] = speaker_name
        
        # è®¾ç½®æ¨¡å‹ç‰ˆæœ¬ï¼ˆv2 æ¨¡å‹ï¼‰
        config["model"]["version"] = "v2"
        
        # è®¾ç½®ä¿å­˜æƒé‡ç›®å½•ï¼ˆprocess_ckpt.py éœ€è¦ï¼‰
        config["save_weight_dir"] = os.path.join(exp_dir, f"logs_s2_{config['model']['version']}")
        
        # è®¾ç½®æ•°æ®è·¯å¾„ï¼ˆexp_dir åº”è¯¥åœ¨ data å­—æ®µä¸‹ï¼‰
        config["data"]["exp_dir"] = exp_dir
        config["data"]["training_files"] = os.path.join(exp_dir, "2-name2text.txt")
        config["data"]["validation_files"] = os.path.join(exp_dir, "2-name2text.txt")
        
        # è®¾ç½® wav æ–‡ä»¶è·¯å¾„ï¼ˆStage 2 éœ€è¦ï¼‰
        config["data"]["wav_path"] = os.path.join(exp_dir, "5-wav32k")
        
        # ç¡®ä¿å¿…éœ€çš„æ–‡ä»¶å’Œç›®å½•å­˜åœ¨
        wav_dir = os.path.join(exp_dir, "5-wav32k")
        if not os.path.exists(wav_dir):
            raise ValueError(f"WAV ç›®å½•ä¸å­˜åœ¨: {wav_dir}ï¼Œè¯·ç¡®ä¿æ•°æ®é¢„å¤„ç†å·²å®Œæˆ")
        
        cnhubert_dir = os.path.join(exp_dir, "4-cnhubert")
        if not os.path.exists(cnhubert_dir):
            raise ValueError(f"HuBERT ç›®å½•ä¸å­˜åœ¨: {cnhubert_dir}ï¼Œè¯·ç¡®ä¿æ•°æ®é¢„å¤„ç†å·²å®Œæˆ")
        
        # ç¡®ä¿ 2-name2text.txt å­˜åœ¨ï¼ˆä» 2-name2text-0.txt å¤åˆ¶ï¼‰
        name2text_src = os.path.join(exp_dir, "2-name2text-0.txt")
        name2text_dst = os.path.join(exp_dir, "2-name2text.txt")
        if not os.path.exists(name2text_dst):
            if os.path.exists(name2text_src):
                shutil.copy2(name2text_src, name2text_dst)
                print(f"   ğŸ“‹ åˆ›å»ºæ–‡æœ¬æ ‡æ³¨æ–‡ä»¶: {name2text_dst}")
            else:
                raise ValueError(f"æ–‡æœ¬æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨: {name2text_src}ï¼Œè¯·ç¡®ä¿æ•°æ®é¢„å¤„ç†å·²å®Œæˆ")
        
        # åˆ›å»º checkpoint ä¿å­˜ç›®å½•
        logs_s2_dir = os.path.join(exp_dir, f"logs_s2_{config['model']['version']}")
        os.makedirs(logs_s2_dir, exist_ok=True)
        print(f"   ğŸ“ åˆ›å»º checkpoint ç›®å½•: {logs_s2_dir}")
        
        # ä¿å­˜ä¸´æ—¶é…ç½®
        temp_config = os.path.join(exp_dir, "s2_config.json")
        with open(temp_config, "w", encoding="utf-8") as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        # æ‰§è¡Œè®­ç»ƒ
        script_s2 = os.path.join(self.sovits_path, "GPT_SoVITS/s2_train.py")
        cmd = [self.python_exec, script_s2, "--config", temp_config]
        
        # è®¾ç½®ç¯å¢ƒå˜é‡å’Œå·¥ä½œç›®å½•
        env = os.environ.copy()
        env["PYTHONPATH"] = self.sovits_path + (f":{env.get('PYTHONPATH', '')}" if env.get('PYTHONPATH') else "")
        
        print(f"   ğŸš€ å¼€å§‹è®­ç»ƒ...")
        print(f"   å‘½ä»¤: {' '.join(cmd)}")
        
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
            env=env,
            cwd=self.sovits_path
        )
        
        # å®æ—¶è¾“å‡ºæ—¥å¿—
        for line in process.stdout:
            print(f"   {line.strip()}")
        
        process.wait()
        
        if process.returncode != 0:
            print(f"   âŒ Stage 2 è®­ç»ƒå¤±è´¥")
            return None
        
        # æŸ¥æ‰¾è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆä½¿ç”¨ v2 æ¨¡å‹è·¯å¾„ï¼‰
        logs_dir = os.path.join(exp_dir, "logs_s2_v2")
        if not os.path.exists(logs_dir):
            print(f"   âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {logs_dir}")
            return None
        
        # ä¼˜å…ˆæŸ¥æ‰¾æœ€ç»ˆå¯¼å‡ºçš„æƒé‡æ–‡ä»¶ï¼ˆåŒ…å« config çš„å®Œæ•´æ¨¡å‹ï¼‰
        # æ ¼å¼å¦‚: ä¸‰å¥¶å¥¶_e8.pth, ä¸‰å¥¶å¥¶_e10.pth
        weight_files = [
            f for f in os.listdir(logs_dir) 
            if f.endswith(".pth") 
            and speaker_name in f 
            and "_e" in f
            and not f.startswith("G_")
            and not f.startswith("D_")
        ]
        
        if weight_files:
            # æ‰¾åˆ°äº†æœ€ç»ˆæƒé‡æ–‡ä»¶ï¼ŒæŒ‰æ–‡ä»¶åä¸­çš„ epoch æ’åº
            weight_files.sort(reverse=True)
            model_path = os.path.join(logs_dir, weight_files[0])
            print(f"   âœ… Stage 2 è®­ç»ƒå®Œæˆ")
            print(f"   ğŸ“¦ æ‰¾åˆ°æœ€ç»ˆæƒé‡æ–‡ä»¶: {model_path}")
            return model_path
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ€ç»ˆæƒé‡æ–‡ä»¶ï¼ŒæŸ¥æ‰¾ checkpoint æ–‡ä»¶
        print(f"   âš ï¸  æœªæ‰¾åˆ°æœ€ç»ˆæƒé‡æ–‡ä»¶ï¼Œå°è¯•ä½¿ç”¨ checkpoint")
        g_files = [f for f in os.listdir(logs_dir) if f.startswith("G_") and f.endswith(".pth")]
        if not g_files:
            print(f"   âŒ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹")
            return None
        
        # æŒ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„
        g_files.sort(key=lambda x: os.path.getmtime(os.path.join(logs_dir, x)), reverse=True)
        checkpoint_path = os.path.join(logs_dir, g_files[0])
        
        # å°è¯•ä» checkpoint è½¬æ¢ä¸ºæƒé‡æ–‡ä»¶
        print(f"   ğŸ”§ ä» checkpoint è½¬æ¢ä¸ºæƒé‡æ–‡ä»¶...")
        weight_path = self._convert_checkpoint_to_weight(
            checkpoint_path, 
            logs_dir, 
            speaker_name, 
            config_path=os.path.join(exp_dir, "s2_config.json")
        )
        
        if weight_path:
            print(f"   âœ… Stage 2 è®­ç»ƒå®Œæˆ")
            print(f"   ğŸ“¦ æ¨¡å‹è·¯å¾„: {weight_path}")
            return weight_path
        else:
            print(f"   âš ï¸  è½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨ checkpointï¼ˆå¯èƒ½æ— æ³•ç”¨äºæ¨ç†ï¼‰")
            print(f"   ğŸ“¦ æ¨¡å‹è·¯å¾„: {checkpoint_path}")
            return checkpoint_path
    
    def train_speaker_complete(
        self,
        speaker_name: str,
        audio_files: List[Dict],
        audio_text_map: Optional[Dict[str, str]] = None,
        s1_epochs: int = 15,
        s2_epochs: int = 10,
        batch_size: int = 8
    ) -> Dict:
        """
        å®Œæ•´è®­ç»ƒæµç¨‹ï¼šæ•°æ®å‡†å¤‡ â†’ é¢„å¤„ç† â†’ Stage 1 â†’ Stage 2
        
        Args:
            speaker_name: è¯´è¯è€…åç§°
            audio_files: éŸ³é¢‘æ–‡ä»¶åˆ—è¡¨
            audio_text_map: éŸ³é¢‘æ–‡æœ¬æ˜ å°„ï¼ˆå¯é€‰ï¼‰
            s1_epochs: Stage 1 è®­ç»ƒè½®æ•°
            s2_epochs: Stage 2 è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        start_time = time.time()
        
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¼€å§‹å®Œæ•´è®­ç»ƒæµç¨‹: {speaker_name}")
        print(f"{'='*60}")
        
        result = {
            "speaker_name": speaker_name,
            "status": "failed",
            "start_time": datetime.now().isoformat(),
            "gpt_model": None,
            "sovits_model": None,
            "exp_dir": None,
            "error": None
        }
        
        try:
            # æ­¥éª¤ 1: å‡†å¤‡æ•°æ®
            exp_dir = self.prepare_training_data(speaker_name, audio_files, audio_text_map)
            result["exp_dir"] = exp_dir
            
            # æ­¥éª¤ 2: æ•°æ®é¢„å¤„ç†
            if not self.run_data_preprocessing(exp_dir, speaker_name):
                result["error"] = "æ•°æ®é¢„å¤„ç†å¤±è´¥"
                return result
            
            # æ­¥éª¤ 3: Stage 1 è®­ç»ƒ
            gpt_model = self.train_stage1_gpt(exp_dir, speaker_name, s1_epochs, batch_size)
            if gpt_model is None:
                result["error"] = "Stage 1 è®­ç»ƒå¤±è´¥"
                return result
            result["gpt_model"] = gpt_model
            
            # æ­¥éª¤ 4: Stage 2 è®­ç»ƒ
            sovits_model = self.train_stage2_sovits(exp_dir, speaker_name, s2_epochs, batch_size)
            if sovits_model is None:
                result["error"] = "Stage 2 è®­ç»ƒå¤±è´¥"
                return result
            result["sovits_model"] = sovits_model
            
            # è®­ç»ƒæˆåŠŸ
            result["status"] = "completed"
            result["end_time"] = datetime.now().isoformat()
            result["duration"] = time.time() - start_time
            
            print(f"\n{'='*60}")
            print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼")
            print(f"   è€—æ—¶: {result['duration']:.1f} ç§’")
            print(f"   GPT æ¨¡å‹: {gpt_model}")
            print(f"   SoVITS æ¨¡å‹: {sovits_model}")
            print(f"{'='*60}\n")
            
        except Exception as e:
            result["error"] = str(e)
            print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        return result


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    trainer = GPTSoVITSTrainer()
    print("âœ… è®­ç»ƒå™¨åˆå§‹åŒ–æˆåŠŸ")

